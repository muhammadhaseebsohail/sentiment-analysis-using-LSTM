{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deployment_IMDb_Movie_Sentiment_Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZdZbHrLBYL-",
        "outputId": "94600802-d932-4519-e6ab-315987096a26"
      },
      "source": [
        "!pip install -q pyngrok\n",
        "\n",
        "!pip install -q streamlit\n",
        "\n",
        "!pip install -q streamlit_ace"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |▍                               | 10 kB 22.1 MB/s eta 0:00:01\r\u001b[K     |▉                               | 20 kB 27.5 MB/s eta 0:00:01\r\u001b[K     |█▎                              | 30 kB 19.9 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 40 kB 16.4 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 51 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 61 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |███                             | 71 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 81 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |████                            | 92 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 102 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 112 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 122 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 133 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 143 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 153 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████                         | 163 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 174 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████                        | 184 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 194 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 204 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 215 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 225 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 235 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 245 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 256 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 266 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 276 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 286 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 296 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 307 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 317 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 327 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 337 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 348 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 358 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 368 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 378 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 389 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 399 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 409 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 419 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 430 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 440 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 450 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 460 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 471 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 481 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 491 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 501 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 512 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 522 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 532 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 542 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 552 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 563 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 573 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 583 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 593 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 604 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 614 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 624 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 634 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 645 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 655 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 665 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 675 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 686 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 696 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 706 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 716 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 727 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 737 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 745 kB 5.3 MB/s \n",
            "\u001b[?25h  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 8.0 MB 5.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 75 kB 4.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 111 kB 60.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.3 MB 55.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 180 kB 61.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 124 kB 60.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 787 kB 50.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 370 kB 45.7 MB/s \n",
            "\u001b[?25h  Building wheel for blinker (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.20 which is incompatible.\n",
            "google-colab 1.0.0 requires ipykernel~=4.10, but you have ipykernel 6.4.1 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.27.0 which is incompatible.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 5.3 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPHZTUd6R6OB",
        "outputId": "723aa3f2-6983-46b7-f1fe-0f4c0f5e8036"
      },
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/NLP/IMDB Dataset.csv')\n",
        "data['review'] = data['review'].str.lower()\n",
        "stopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \n",
        "             \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\",\n",
        "             \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \n",
        "             \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\",\n",
        "             \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\",\n",
        "             \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \n",
        "             \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\",\n",
        "             \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\",\n",
        "             \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\",\n",
        "             \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\",\n",
        "             \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n",
        "\n",
        "def remove_stopwords(data):\n",
        "  data['review without stopwords'] = data['review'].apply(lambda x : ' '.join([word for word in x.split() if word not in (stopwords)]))\n",
        "  return data\n",
        "\n",
        "def remove_tags(string):\n",
        "    result = re.sub('<.*?>','',string)\n",
        "    return result\n",
        "\n",
        "data_without_stopwords = remove_stopwords(data)\n",
        "data_without_stopwords['clean_review']= data_without_stopwords['review without stopwords'].apply(lambda cw : remove_tags(cw))\n",
        "data_without_stopwords['clean_review'] = data_without_stopwords['clean_review'].str.replace('[{}]'.format(string.punctuation), ' ')\n",
        "\n",
        "reviews = data_without_stopwords['clean_review']\n",
        "\n",
        "reviews_list = []\n",
        "for i in range(len(reviews)):\n",
        "  reviews_list.append(reviews[i])\n",
        "\n",
        "sentiment = data_without_stopwords['sentiment']\n",
        "y = np.array(list(map(lambda x: 1 if x==\"positive\" else 0, sentiment)))\n",
        "\n",
        "X_train, X_test,Y_train, Y_test = train_test_split(reviews_list, y, test_size=0.2, random_state = 45)\n",
        "\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "@st.cache(allow_output_mutation=True)\n",
        "def load_model():\n",
        "  model=tf.keras.models.load_model(\"/content/drive/MyDrive/Colab Notebooks/NLP/imdb_lstm.h5\")\n",
        "  return model\n",
        "with st.spinner('Model is being loaded..'):\n",
        "  model=load_model()\n",
        "\n",
        "st.write(\"\"\"\n",
        "         # Sentiment Analysis With LSTM\n",
        "         \"\"\"\n",
        "         )\n",
        "\n",
        "# file = st.file_uploader(\"Please upload an brain scan file\", type=[\"jpg\", \"png\"])\n",
        "# import cv2\n",
        "# from PIL import Image, ImageOps\n",
        "# import numpy as np\n",
        "text = st.text_input(\"Please Enter Your Statement...\")\n",
        "st.set_option('deprecation.showfileUploaderEncoding', False)\n",
        "def import_and_predict(sentence, model):\n",
        "        X_test_indices = tokenizer.texts_to_sequences([sentence])\n",
        "        print(X_test_indices)\n",
        "        X_test_indices = tf.keras.preprocessing.sequence.pad_sequences(X_test_indices, maxlen=150, padding='post')\n",
        "        print(X_test_indices)\n",
        "        predict = model.predict(X_test_indices)\n",
        "        if predict > 0.5:\n",
        "          probabilty = predict\n",
        "          prediction = \"Positive\"\n",
        "        else: \n",
        "          probabilty = predict\n",
        "          prediction = \"Negative\"\n",
        "          \n",
        "        # size = (180,180)    \n",
        "        # image = ImageOps.fit(image_data, size, Image.ANTIALIAS)\n",
        "        # image = np.asarray(image)\n",
        "        # img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        # #img_resize = (cv2.resize(img, dsize=(75, 75),    interpolation=cv2.INTER_CUBIC))/255.\n",
        "        \n",
        "        # img_reshape = img[np.newaxis,...]\n",
        "    \n",
        "        # prediction = model.predict(img_reshape)\n",
        "        \n",
        "        return probabilty, prediction\n",
        "if text is None:\n",
        "    st.text(\"Please Enter an Statement \")\n",
        "else:\n",
        "    # image = Image.open(file)\n",
        "    # st.image(image, use_column_width=True)\n",
        "    probabilty, prediction = import_and_predict(text, model)\n",
        "    # score = tf.nn.softmax(predictions[0])\n",
        "    # st.write(probabilty)\n",
        "    # st.write(prediction)\n",
        "    string = f\"This Statement most likely is {prediction} with probabilty of {probabilty}...\"\n",
        "    st.success(string)\n",
        "    # print(\n",
        "    # \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
        "    # .format(class_names[np.argmax(score)], 100 * np.max(score)))\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4mwLb4ZBvWk"
      },
      "source": [
        "!streamlit run app.py &>/dev/null&"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkOF0HrdBwxI",
        "outputId": "5e8c4399-c367-4917-dc32-e2136e5d4612"
      },
      "source": [
        "from pyngrok import ngrok\n",
        " \n",
        "public_url = ngrok.connect('8501')\n",
        "public_url"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ""
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<NgrokTunnel: \"http://cff0-34-122-82-153.ngrok.io\" -> \"http://localhost:8501\">"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KN_DCbggBzer"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}